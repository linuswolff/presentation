---
title: "Diabetes Dataset Testing"
output: html_document
date: "2023-09-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mlbench)
library(caret)
library(glmnet)
library(mice)
library(performanceEstimation)
library(performance)
library(knitr)
library(kableExtra)
```
### Functions
```{r}
count_na_per_column <- function(df) {
    sapply(df, function(x) sum(is.na(x)))
}
```
### Data
```{r}
# Loading the data
data("PimaIndiansDiabetes2", package = "mlbench")
diabetes <- PimaIndiansDiabetes2
diabetes$diabetes <- as.factor(ifelse(diabetes$diabetes == "pos", 1, 0))
count_na_per_column(diabetes)
diabetes <- na.omit(diabetes)
```
### Data for models
```{r}
# Split
set.seed(123)
n <- nrow(diabetes)
training.samples <- sample(1:n, size = 0.8 * n)
train.data <- diabetes[training.samples, ]
#train.data <- smote(diabetes ~ ., train.data, perc.over = 1)
test.data <- diabetes[-training.samples, ]
test.data <- na.omit(test.data)


  
# Handle NA's in training set
#mice <- complete(mice(subset(train.data, select = -c(triceps, insulin)), method='rf', seed = 123))
#mice <- complete(mice(train.data, method='rf', seed = 123))

#train.data$glucose <- mice$glucose
#train.data$pressure <- mice$pressure
#train.data$mass <- mice$mass

#train.data <- na.omit(train.data)

#mice.triceps <- complete(mice(subset(train.data, select = -insulin), method='rf', seed = 123))
#train.data$triceps <- mice.triceps$triceps

#mice.insulin <- complete(mice(train.data, method='rf', seed = 123))
#train.data$insulin <- mice.insulin$insulin

# Train
X.train <- model.matrix(diabetes~., data = train.data)[,-1]
X.train <- scale(X.train)
y.train <- train.data$diabetes

# Test
X.test <- model.matrix(diabetes ~., data = test.data)[,-1]
X.test <- scale(X.test)
y.test <- test.data$diabetes
```
### Logistic Regression
```{r}
set.seed(123)
log.model <- glm(diabetes ~., data = train.data, family = "binomial")

# Make predictions
probabilities <- predict(log.model, newdata = test.data, type = "response")
predicted.classes <- as.factor(ifelse(probabilities > 0.5, 1, 0))

# Accuracy
# mean(predicted.classes == y.test)
confusionMatrix(predicted.classes, y.test, positive = "1")
```
### LASSO
```{r}
set.seed(123)
cv.lasso.model <- cv.glmnet(X.train, y.train, alpha = 1, family = "binomial", intercept = T)
#plot(cv.lasso.model)
cbind(coef(cv.lasso.model, s = cv.lasso.model$lambda.min), coef(cv.lasso.model, s = cv.lasso.model$lambda.1se))



# Make predictions
probabilities <- predict(cv.lasso.model, newx = X.test, s = cv.lasso.model$lambda.min, type = "response")
predicted.classes <- as.factor(ifelse(probabilities > 0.5, 1, 0))

# Accuracy
#mean(predicted.classes == y.test)
confusionMatrix(predicted.classes, y.test, positive = "1")
```
### Ridge
```{r}
set.seed(123)
cv.ridge.model <- cv.glmnet(X.train, y.train, alpha = 0, family = "binomial", intercept = T)
#plot(cv.ridge)

# Make predictions
probabilities <- predict(cv.ridge.model, newx = X.test, s = cv.ridge.model$lambda.min, type = "response")
predicted.classes <- as.factor(ifelse(probabilities > 0.5, 1, 0))

# Accuracy
#mean(predicted.classes == y.test)
confusionMatrix(as.factor(predicted.classes), y.test, positive = "1")
```
### Elastic Net
```{r}
set.seed(123)

# CV and tuning grid
cvControl <- trainControl(method = "cv", number = 10)
tuneGrid <- expand.grid(alpha = seq(0, 1, by = 0.05), lambda = 10^seq(2, -2, length=100))

# Train model
elasticnet.model <- train(X.train, y.train, method = "glmnet", trControl = cvControl, 
                          tuneGrid = tuneGrid, intercept = T)

# Make predictions
probabilities <- predict(elasticnet.model, newdata = X.test, type = "prob") # "raw" already outputs predicted.class
predicted.classes <- ifelse(probabilities$`1` > 0.5, 1, 0)

# Accuracy
# mean(predicted.classes == y.test)
confusionMatrix(as.factor(predicted.classes), y.test, positive = "1")

# Output coefficients
coef(elasticnet.model$finalModel, s = elasticnet.model$bestTune$lambda)
```

```{r}
library(ggcorrplot)
# Ensure that all remaining columns after subsetting are numeric
numeric_data <- subset(diabetes, select = -c(diabetes))
corr <- round(cor(numeric_data), 1)
ggcorrplot(corr,
           type = "lower",
           lab = TRUE,
           lab_size = 3,
           colors = c("red", "white", "cyan4"),
           title = "Correlogram of Diabetes Dataset",
           ggtheme = theme_bw())

```

```{r}
check_collinearity(log.model)
```
